# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MjDJan0Tk6HRvdXcIs4YjGoTYLkycnrs

# **PROYEK PREDICTIVE ANALYTICS - RED WINE QUALITY PREDICTION**

## Import Libraries
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.inspection import permutation_importance
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                           classification_report, confusion_matrix)
from scipy import stats
import warnings

warnings.filterwarnings('ignore')
sns.set(style='whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

print("Libraries imported successfully!")

"""- **Metode yang digunakan**
  Pada tahap awal ini, metode yang digunakan adalah mengimpor serangkaian *library* (pustaka) fundamental Python untuk analisis data dan *machine learning*. Library yang diimpor meliputi:
  - **Pandas** dan **NumPy** untuk manipulasi dan komputasi data.
  - **Matplotlib** dan **Seaborn** untuk visualisasi data.
  - **Scikit-learn (sklearn)** untuk pemodelan *machine learning*, termasuk pembagian data, prapemrosesan, algoritma klasifikasi (Logistic Regression, Random Forest, SVM), serta evaluasi metrik.
  - **SciPy** untuk komputasi statistik.
  - **Warnings** untuk mengelola dan menyembunyikan pesan peringatan yang tidak kritikal.

- **Alasan penggunaan**
  Setiap *library* dipilih karena perannya yang krusial dalam alur kerja proyek data science.
  - **Pandas & NumPy** adalah standar industri untuk memuat, membersihkan, dan mentransformasi data tabular secara efisien.
  - **Matplotlib & Seaborn** digunakan untuk melakukan *Exploratory Data Analysis* (EDA) secara visual, yang membantu dalam mengidentifikasi pola, anomali, dan hubungan antar variabel.
  - **Scikit-learn** menyediakan *toolkit* yang komprehensif dan terstandardisasi untuk membangun, melatih, dan mengevaluasi model prediktif, yang menjadi inti dari proyek ini.
  - **SciPy** disiapkan untuk mendukung analisis statistik yang lebih mendalam jika diperlukan.
  - **Warnings** diimpor untuk menjaga kebersihan *output* notebook, agar fokus tetap pada hasil analisis.

- **Insight dan Hasil yang didapat**
  - **Hasil:** *Output* `Libraries imported successfully!` mengonfirmasi bahwa semua *library* yang dibutuhkan untuk proyek ini telah ter-install dengan benar di lingkungan kerja dan siap untuk digunakan pada tahap-tahap selanjutnya.
  - **Insight:** Pemilihan *library* ini memberikan gambaran awal mengenai metodologi proyek yang akan dijalankan. Proyek ini jelas akan melibatkan tahapan EDA, prapemrosesan data, pemodelan dengan beberapa algoritma klasifikasi, dan evaluasi performa model. Ini menunjukkan sebuah pendekatan analisis yang terstruktur dan komprehensif.

## Data Understanding

### Load Data
"""

# Data Understanding - Load Data
file_path = './datasets/winequality-red.csv'
try:
    df = pd.read_csv(file_path, sep=';')
    print("Dataset loaded successfully!")
    print("Shape of dataset:", df.shape)
except FileNotFoundError:
    print(f"Error: File '{file_path}' not found. Please check the file path.")

if 'df' in locals():
    print("\nFirst 5 rows of the dataset:")
    print(df.head())

    print("\nDataset Info:")
    df.info()

    print("\nDescriptive Statistics:")
    print(df.describe())

    print("\nChecking for missing values:")
    print(df.isnull().sum())

    print("\nChecking for duplicate rows:")
    print(df.duplicated().sum())

    print("\nUnique values in each column:")
    for column in df.columns:
        print(f"{column}: {df[column].nunique()} unique values")

else:
    print("Dataset could not be loaded. Please check the file path and try again.")

"""- **Metode yang digunakan**
  Pada tahap ini, dataset dimuat menggunakan `pandas.read_csv()`. Setelah itu, serangkaian fungsi pandas digunakan untuk melakukan inspeksi awal, yaitu:
  - **`.shape`**: Untuk melihat dimensi dataset (baris dan kolom).
  - **`.head()`**: Untuk menampilkan 5 baris pertama dan memahami struktur data.
  - **`.info()`**: Untuk mendapatkan ringkasan teknis, termasuk tipe data setiap kolom dan jumlah data non-null.
  - **`.describe()`**: Untuk menghasilkan statistik deskriptif (seperti rata-rata, standar deviasi, nilai minimum/maksimum) untuk setiap kolom numerik.
  - **`.isnull().sum()`** dan **`.duplicated().sum()`**: Untuk memeriksa kualitas data dengan menghitung nilai yang hilang (kosong) dan baris data yang duplikat.
  - **`.nunique()`**: Untuk menghitung jumlah nilai unik di setiap kolom.

- **Alasan penggunaan**
  Tahap *data understanding* ini adalah fondasi dari setiap proyek analisis data. Metode-metode ini dipilih karena memberikan gambaran umum yang cepat dan komprehensif tentang dataset.
  - **Verifikasi Data**: Memastikan data berhasil dimuat dengan benar dan sesuai ekspektasi.
  - **Penilaian Kualitas**: Mengidentifikasi masalah umum seperti data yang hilang atau duplikat sejak dini. Masalah ini harus ditangani pada tahap *data cleaning* agar tidak memengaruhi performa model.
  - **Pemahaman Fitur**: Memahami tipe data dan distribusi statistik setiap fitur. Ini membantu dalam menentukan langkah pra-pemrosesan selanjutnya, seperti perlunya *feature scaling* (penyekalaan fitur) jika rentang nilai antar kolom sangat berbeda.
  - **Identifikasi Target**: Memahami sifat dari kolom target (dalam hal ini 'quality'), yang penting untuk menentukan jenis masalah (misalnya, klasifikasi atau regresi).

- **Insight dan Hasil yang didapat**
  Dari eksekusi kode, didapatkan beberapa *insight* penting:
  - **Struktur Data**: Dataset berhasil dimuat dan terdiri dari **1599 baris** dan **12 kolom**. Semua kolom, kecuali `quality`, memiliki tipe data `float64`, sedangkan `quality` adalah `int64`.
  - **Kualitas Data**: Tidak ditemukan **nilai yang hilang** (*missing values*) sama sekali, yang menyederhanakan proses pembersihan data. Namun, terdeteksi adanya **240 baris duplikat**. Data duplikat ini perlu ditangani (kemungkinan besar dihapus) pada tahap selanjutnya untuk menghindari bias pada model.
  - **Variabel Target**: Kolom `quality` memiliki **6 nilai unik**, yang mengonfirmasi bahwa ini adalah variabel target untuk masalah **klasifikasi**.
  - **Distribusi Fitur**: Statistik deskriptif menunjukkan bahwa rentang nilai antar fitur sangat bervariasi (misalnya, `total sulfur dioxide` memiliki `max` 289, sementara `chlorides` memiliki `max` 0.611). Hal ini mengindikasikan kuat bahwa **feature scaling** akan menjadi langkah pra-pemrosesan yang krusial sebelum melatih model seperti *Logistic Regression* atau SVM.

### EDA (Exploratory Data Analysis)
"""

# EDA - Before Preprocessing
if 'df' in locals():
    # Distribusi variabel target 'quality' sebelum transformasi
    plt.figure(figsize=(10, 6))
    sns.countplot(x='quality', data=df, palette='viridis')
    plt.title('Distribution of Wine Quality (Original)')
    plt.xlabel('Quality Score')
    plt.ylabel('Count')
    plt.show()

    # Membuat kategori biner untuk 'quality'
    df['quality_category'] = df['quality'].apply(lambda x: 1 if x > 5 else 0)

    # Distribusi variabel target setelah transformasi
    plt.figure(figsize=(8, 5))
    sns.countplot(x='quality_category', data=df, palette='pastel')
    plt.title('Distribution of Wine Quality (Binary: 0=Not Good, 1=Good)')
    plt.xlabel('Quality Category')
    plt.ylabel('Count')
    plt.xticks([0, 1], ['Not Good (≤5)', 'Good (>5)'])
    plt.show()

    print("\nValue counts for 'quality_category':")
    class_counts = df['quality_category'].value_counts()
    print(class_counts)
    print(f"Class imbalance ratio (Not Good : Good): {class_counts[0] / class_counts[1]:.2f}")

    # Distribusi fitur numerik
    numerical_features = df.select_dtypes(include=np.number).columns.tolist()
    features_for_hist = [col for col in numerical_features if col not in ['quality', 'quality_category']]

    if len(features_for_hist) > 0:
        fig, axes = plt.subplots(4, 3, figsize=(15, 12))
        axes = axes.ravel()

        for idx, feature in enumerate(features_for_hist):
            axes[idx].hist(df[feature], bins=20, edgecolor='black', alpha=0.7)
            axes[idx].set_title(f'Distribution of {feature}')
            axes[idx].set_xlabel(feature)
            axes[idx].set_ylabel('Frequency')

        # Hide unused subplots
        for idx in range(len(features_for_hist), len(axes)):
            axes[idx].set_visible(False)

        plt.suptitle('Distribution of Features (Before Preprocessing)', y=1.02)
        plt.tight_layout()
        plt.show()

    # Boxplot untuk deteksi outliers
    fig, axes = plt.subplots(4, 3, figsize=(15, 12))
    axes = axes.ravel()

    for idx, feature in enumerate(features_for_hist):
        sns.boxplot(y=df[feature], ax=axes[idx])
        axes[idx].set_title(f'Boxplot of {feature}')

    for idx in range(len(features_for_hist), len(axes)):
        axes[idx].set_visible(False)

    plt.suptitle('Boxplots for Outlier Detection (Before Preprocessing)', y=1.02)
    plt.tight_layout()
    plt.show()

    # Korelasi antar fitur
    plt.figure(figsize=(14, 10))
    correlation_matrix = df.corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f",
                linewidths=0.5, annot_kws={"size": 8})
    plt.title('Correlation Matrix of Features')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # Hubungan fitur dengan target
    fig, axes = plt.subplots(4, 3, figsize=(15, 12))
    axes = axes.ravel()

    for idx, feature in enumerate(features_for_hist):
        sns.boxplot(x='quality_category', y=feature, data=df, ax=axes[idx])
        axes[idx].set_title(f'{feature} vs Quality Category')
        axes[idx].set_xticklabels(['Not Good (≤5)', 'Good (>5)'])

    for idx in range(len(features_for_hist), len(axes)):
        axes[idx].set_visible(False)

    plt.suptitle('Feature Distribution by Quality Category', y=1.02)
    plt.tight_layout()
    plt.show()

else:
    print("Dataset 'df' not loaded. Run previous cells.")

"""Tahap ini bertujuan untuk menggali lebih dalam dataset guna memahami karakteristik, menemukan pola, mengidentifikasi anomali, dan melihat hubungan antar variabel melalui visualisasi.

#### **1. Distribusi Variabel Target**
- **Metode yang digunakan**
  Menggunakan `countplot` dari library Seaborn untuk memvisualisasikan jumlah sampel untuk setiap kategori kualitas. [cite_start]Metode ini diterapkan pada variabel `quality` asli (skala 3-8) [cite: 1] dan pada variabel `quality_category` baru yang merupakan hasil transformasi biner (0 untuk "Not Good", 1 untuk "Good").

- **Alasan penggunaan**
  Visualisasi ini penting untuk memahami sebaran kelas pada variabel target. Transformasi menjadi kategori biner dilakukan untuk menyederhanakan masalah dari klasifikasi multi-kelas menjadi klasifikasi biner, yang seringkali lebih mudah ditangani dan dapat menghasilkan model yang lebih robust, terutama ketika beberapa kelas asli memiliki jumlah sampel yang sangat sedikit.

- **Insight dan Hasil yang didapat**
  - [cite_start]**Distribusi Asli[cite: 1]:** Sebaran kualitas anggur tidak merata. Mayoritas sampel terkonsentrasi pada kualitas skor 5 dan 6. Kategori kualitas sangat rendah (3, 4) dan sangat tinggi (8) memiliki jumlah sampel yang sangat sedikit, yang dapat menyulitkan model klasifikasi multi-kelas.
  - **Distribusi Biner:** Setelah diubah menjadi dua kategori ("Good" untuk skor > 5 dan "Not Good" untuk skor ≤ 5), distribusi kelas menjadi jauh lebih seimbang. Terdapat 855 sampel "Good" dan 744 sampel "Not Good". Kondisi ini ideal untuk melatih model klasifikasi biner.

---
#### **2. Distribusi Fitur Numerik**
- **Metode yang digunakan**
  **Histogram** dibuat untuk setiap fitur numerik untuk memvisualisasikan distribusi atau sebaran frekuensi nilainya.

- **Alasan penggunaan**
  Histogram membantu mengidentifikasi bentuk distribusi data (misalnya, normal atau miring/skewed). Informasi ini krusial untuk menentukan apakah diperlukan transformasi data (seperti log-transform) dan untuk memilih teknik *feature scaling* yang tepat (misalnya, *Standardization* lebih cocok untuk data terdistribusi normal).

- **Insight dan Hasil yang didapat**
  - Sebagian besar fitur, seperti `volatile acidity`, `residual sugar`, `chlorides`, dan `total sulfur dioxide`, memiliki distribusi **miring ke kanan (*right-skewed*)**. Ini menunjukkan bahwa mayoritas anggur memiliki nilai rendah untuk fitur-fitur ini, dengan beberapa nilai ekstrem yang tinggi.
  - Fitur `density` dan `pH` menunjukkan distribusi yang paling mendekati **distribusi normal (simetris)**.
  - `alcohol` memiliki distribusi yang sedikit miring ke kiri (*left-skewed*).

---
#### **3. Deteksi Outliers**
- **Metode yang digunakan**
  **Box plot** digunakan untuk setiap fitur guna mengidentifikasi adanya pencilan (*outliers*), yaitu data yang nilainya jauh berbeda dari sebagian besar data lainnya.

- **Alasan penggunaan**
  *Outliers* dapat secara signifikan memengaruhi perhitungan statistik dan performa model, terutama model linier. Mendeteksi *outliers* di tahap awal memungkinkan kita untuk merencanakan strategi penanganannya (misalnya, menghapus, mengubah, atau menggunakan model yang robust terhadap *outliers*).

- **Insight dan Hasil yang didapat**
  - Hampir semua fitur menunjukkan adanya *outliers* (titik-titik di luar "kumis" plot).
  - Fitur seperti `residual sugar`, `chlorides`, dan `total sulfur dioxide` memiliki jumlah *outliers* yang paling signifikan dan ekstrem.
  - Kehadiran *outliers* ini mengonfirmasi bahwa penanganan *outliers* akan menjadi bagian penting dari tahap *data preprocessing*.

---
#### **4. Analisis Korelasi Antar Fitur**
- **Metode yang digunakan**
  **Matriks Korelasi (*Correlation Matrix*)** yang divisualisasikan menggunakan **Heatmap**. Metode ini menghitung koefisien korelasi Pearson antara setiap pasang fitur, di mana nilai mendekati 1 atau -1 menunjukkan hubungan linear yang kuat.

- **Alasan penggunaan**
  Heatmap korelasi berguna untuk dua hal utama:
  1.  Menemukan fitur yang paling berhubungan dengan variabel target (`quality_category`), yang berpotensi menjadi prediktor yang baik.
  2.  Mendeteksi **multikolinearitas**, yaitu korelasi yang tinggi antar sesama fitur prediktor. Multikolinearitas yang tinggi dapat membuat interpretasi model menjadi sulit.

- **Insight dan Hasil yang didapat**
  - **Korelasi dengan Target:** `alcohol` memiliki korelasi positif tertinggi dengan `quality_category` (0.43), diikuti oleh `sulphates` (0.22) dan `citric acid` (0.16). `volatile acidity` memiliki korelasi negatif terkuat (-0.32). Ini menunjukkan fitur-fitur tersebut adalah kandidat prediktor yang paling penting.
  - **Multikolinearitas:** Terdapat korelasi yang cukup kuat antara beberapa fitur, seperti `fixed acidity` dengan `citric acid` (0.67) dan `fixed acidity` dengan `density` (0.67). Hal ini perlu diperhatikan saat memilih model.

---
#### **5. Hubungan Fitur dengan Kualitas**
- **Metode yang digunakan**
  **Box plot berkelompok (*Grouped Box Plots*)** digunakan untuk membandingkan distribusi nilai dari setiap fitur pada kedua kategori kualitas ("Good" dan "Not Good").

- **Alasan penggunaan**
  Metode ini sangat efektif untuk secara visual memvalidasi apakah sebuah fitur dapat membedakan antara kelas target. Jika distribusi (kotak plot) untuk kedua kelas terlihat sangat berbeda, maka fitur tersebut memiliki daya pembeda (*discriminative power*) yang baik.

- **Insight dan Hasil yang didapat**
  (Berdasarkan analisis korelasi dan prinsip umum, karena gambar untuk poin ini tidak disertakan)
  - Fitur dengan daya pembeda tinggi seperti `alcohol` akan menunjukkan kotak plot untuk kategori "Good" berada pada level yang lebih tinggi daripada kategori "Not Good".
  - Fitur `volatile acidity` akan menunjukkan kebalikannya, di mana kategori "Good" akan memiliki level yang lebih rendah.
  - Fitur dengan korelasi rendah seperti `residual sugar` kemungkinan besar akan memiliki kotak plot yang sangat tumpang tindih antara kedua kategori, menandakan daya pembedanya yang rendah.

## Data Preparation
"""

# Data Preparation
if 'df' in locals():
    print("=== DATA PREPARATION ===")

    # 1. Remove duplicates
    print(f"Number of rows before removing duplicates: {len(df)}")
    df_clean = df.drop_duplicates()
    print(f"Number of rows after removing duplicates: {len(df_clean)}")

    # 2. Outlier Detection and Treatment using IQR method
    print("\n=== OUTLIER DETECTION AND TREATMENT ===")
    features_to_clean = [col for col in df_clean.columns if col not in ['quality', 'quality_category']]

    # Calculate outliers before treatment
    outliers_before = {}
    for feature in features_to_clean:
        Q1 = df_clean[feature].quantile(0.25)
        Q3 = df_clean[feature].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df_clean[(df_clean[feature] < lower_bound) | (df_clean[feature] > upper_bound)]
        outliers_before[feature] = len(outliers)

    print("Outliers count before treatment:")
    for feature, count in outliers_before.items():
        print(f"{feature}: {count} outliers")

    # Apply outlier capping (winsorization)
    df_processed = df_clean.copy()

    for feature in features_to_clean:
        Q1 = df_processed[feature].quantile(0.25)
        Q3 = df_processed[feature].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Cap outliers
        df_processed[feature] = np.where(df_processed[feature] < lower_bound, lower_bound, df_processed[feature])
        df_processed[feature] = np.where(df_processed[feature] > upper_bound, upper_bound, df_processed[feature])

    # Calculate outliers after treatment
    outliers_after = {}
    for feature in features_to_clean:
        Q1 = df_processed[feature].quantile(0.25)
        Q3 = df_processed[feature].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df_processed[(df_processed[feature] < lower_bound) | (df_processed[feature] > upper_bound)]
        outliers_after[feature] = len(outliers)

    print("\nOutliers count after treatment:")
    for feature, count in outliers_after.items():
        print(f"{feature}: {count} outliers")

    # 3. Final dataset preparation
    X = df_processed.drop(['quality', 'quality_category'], axis=1)
    y = df_processed['quality_category']

    print(f"\nFinal dataset shape: {df_processed.shape}")
    print("Features (X) and target (y) separated.")
    print("Shape of X:", X.shape)
    print("Shape of y:", y.shape)

    # Check class distribution after preprocessing
    print("\nClass distribution after preprocessing:")
    class_dist = y.value_counts(normalize=True)
    print(class_dist)

    # 4. Train-test split with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nData split completed:")
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    print("\nClass distribution in training set:")
    print(y_train.value_counts(normalize=True))
    print("\nClass distribution in test set:")
    print(y_test.value_counts(normalize=True))

    # 5. Feature Scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Convert back to DataFrame for convenience
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

    print("\nFeature scaling completed.")
    print("Scaling statistics (mean and std of training features after scaling):")
    print(f"Mean: {X_train_scaled_df.mean().round(6).tolist()}")
    print(f"Std: {X_train_scaled_df.std().round(6).tolist()}")

else:
    print("Dataset not available. Run previous cells.")

"""Tahap ini berfokus pada pembersihan dan transformasi data mentah menjadi format yang bersih, terstruktur, dan siap untuk digunakan dalam pemodelan *machine learning*.

#### **1. Menghapus Data Duplikat**
- **Metode yang digunakan**
  Menggunakan fungsi `pandas.drop_duplicates()` untuk mengidentifikasi dan menghapus semua baris yang memiliki nilai identik di seluruh kolomnya.

- **Alasan penggunaan**
  Data duplikat dapat memberikan bobot yang tidak semestinya pada sampel tertentu, yang berpotensi menyebabkan model menjadi bias dan evaluasi performa menjadi tidak akurat. Menghapus duplikat adalah praktik standar untuk memastikan setiap data poin yang digunakan untuk melatih model bersifat unik.

- **Insight dan Hasil yang didapat**
  - Dari total 1599 baris data awal, ditemukan dan dihapus sebanyak **240 baris duplikat**.
  - Dataset yang bersih kini terdiri dari **1359 baris unik**, yang menjadi dasar untuk pemrosesan selanjutnya.

---
#### **2. Penanganan Outliers**
- **Metode yang digunakan**
  Menggunakan metode **IQR (Interquartile Range)** untuk mendeteksi *outliers*. Nilai yang berada di luar batas `Q1 - 1.5 * IQR` dan `Q3 + 1.5 * IQR` dianggap sebagai *outlier*. Sebagai penanganannya, diterapkan teknik **Capping (Winsorization)**, di mana nilai *outlier* tersebut diganti dengan nilai batas IQR-nya.

- **Alasan penggunaan**
  *Outliers* atau nilai ekstrem dapat mengganggu distribusi data dan menurunkan performa model, terutama model yang sensitif terhadap skala seperti regresi logistik. Metode *capping* dipilih sebagai alternatif dari penghapusan agar tidak kehilangan baris data, namun tetap bisa meredam pengaruh buruk dari nilai-nilai ekstrem tersebut.

- **Insight dan Hasil yang didapat**
  - Sebelum penanganan, hampir seluruh fitur terdeteksi memiliki *outliers*.
  - Setelah metode *capping* diterapkan, **jumlah *outliers* menjadi 0** untuk semua fitur. Ini menandakan data sekarang memiliki distribusi yang lebih "terpusat" dan siap untuk tahap selanjutnya.

---
#### **3. Pemisahan Fitur dan Target**
- **Metode yang digunakan**
  Dataset yang telah bersih (`df_processed`) dipisahkan menjadi dua bagian:
  1.  `X`: DataFrame berisi fitur-fitur independen (11 kolom prediktor).
  2.  `y`: Series berisi variabel target dependen (`quality_category`).

- **Alasan penggunaan**
  Ini adalah langkah fundamental dalam *supervised machine learning* untuk membedakan secara jelas mana variabel input yang akan digunakan model untuk belajar (`X`) dan mana variabel output yang harus diprediksi (`y`).

- **Insight dan Hasil yang didapat**
  Dataset berhasil dipisahkan dengan `X` berdimensi (1359, 11) dan `y` berdimensi (1359,). Distribusi kelas pada `y` tetap terjaga, yaitu sekitar 53% untuk kelas 'Good' dan 47% untuk kelas 'Not Good'.

---
#### **4. Pembagian Data Latih dan Uji (Train-Test Split)**
- **Metode yang digunakan**
  Menggunakan fungsi `train_test_split` dari Scikit-learn untuk membagi data menjadi 80% data latih dan 20% data uji. Parameter `stratify=y` juga diaktifkan.

- **Alasan penggunaan**
  - **Evaluasi Objektif**: Model dilatih pada data latih dan diuji pada data uji yang belum pernah "dilihat" sebelumnya. Ini penting untuk mendapatkan estimasi performa model yang tidak bias pada data baru.
  - **Stratifikasi**: Menggunakan `stratify=y` memastikan proporsi kelas pada data latih dan data uji sama seperti proporsi pada dataset keseluruhan. Ini sangat krusial untuk mencegah model dilatih pada distribusi kelas yang berbeda dari data uji.

- **Insight dan Hasil yang didapat**
  - Data telah terbagi menjadi **1087 sampel latih** dan **272 sampel uji**.
  - Output menunjukkan bahwa distribusi kelas pada kedua set data (latih dan uji) hampir identik, membuktikan bahwa proses stratifikasi berhasil.

---
#### **5. Penyekalaan Fitur (Feature Scaling)**
- **Metode yang digunakan**
  **Standardisasi** menggunakan `StandardScaler` dari Scikit-learn. *Scaler* ini dilatih (`fit`) hanya pada data latih, lalu digunakan untuk mentransformasi (`transform`) data latih dan data uji.

- **Alasan penggunaan**
  Fitur-fitur dalam dataset memiliki skala dan rentang nilai yang sangat berbeda. Standardisasi mengubah setiap fitur agar memiliki rata-rata 0 dan standar deviasi 1. Ini adalah langkah wajib untuk algoritma yang sensitif terhadap magnitudo fitur (seperti SVM dan Regresi Logistik) agar dapat bekerja secara optimal dan konvergen dengan baik.

- **Insight dan Hasil yang didapat**
  - Seluruh fitur pada `X_train` dan `X_test` kini memiliki skala yang seragam.
  - Output mengonfirmasi bahwa setelah *scaling*, rata-rata dari setiap fitur pada data latih menjadi mendekati **0** dan standar deviasinya menjadi **1**. Data kini sepenuhnya siap untuk tahap pemodelan.

### EDA (Exploratory Data Analysis) - After Preprocessing
"""

if 'df_processed' in locals():
    print("=== EDA AFTER PREPROCESSING ===")

    # Distribution comparison: Before vs After preprocessing
    features_to_compare = [col for col in df_processed.columns if col not in ['quality', 'quality_category']]

    # Create subplots for before/after comparison
    fig, axes = plt.subplots(len(features_to_compare), 2, figsize=(15, 4*len(features_to_compare)))

    for i, feature in enumerate(features_to_compare):
        # Before preprocessing
        axes[i, 0].hist(df[feature], bins=20, alpha=0.7, color='red', edgecolor='black')
        axes[i, 0].set_title(f'{feature} - Before Preprocessing')
        axes[i, 0].set_xlabel(feature)
        axes[i, 0].set_ylabel('Frequency')

        # After preprocessing
        axes[i, 1].hist(df_processed[feature], bins=20, alpha=0.7, color='blue', edgecolor='black')
        axes[i, 1].set_title(f'{feature} - After Preprocessing')
        axes[i, 1].set_xlabel(feature)
        axes[i, 1].set_ylabel('Frequency')

    plt.suptitle('Feature Distributions: Before vs After Preprocessing', y=1.02)
    plt.tight_layout()
    plt.show()

    # Boxplot comparison (Corrected Section)
    fig, axes = plt.subplots(len(features_to_compare), 2, figsize=(15, 4*len(features_to_compare)))

    for i, feature in enumerate(features_to_compare):
        # Before preprocessing
        sns.boxplot(y=df[feature], ax=axes[i, 0], color='red', boxprops={'alpha': 0.7})
        axes[i, 0].set_title(f'{feature} - Before Preprocessing')

        # After preprocessing
        sns.boxplot(y=df_processed[feature], ax=axes[i, 1], color='blue', boxprops={'alpha': 0.7})
        axes[i, 1].set_title(f'{feature} - After Preprocessing')

    plt.suptitle('Boxplots: Before vs After Preprocessing', y=1.0) # Adjusted y for better title placement
    plt.tight_layout(rect=[0, 0.03, 1, 0.99]) # Adjusted layout to prevent title overlap
    plt.show()

    # Distribution of scaled features
    X_train_scaled_df.hist(bins=20, figsize=(15, 10), layout=(4, 3), edgecolor='black')
    plt.suptitle('Distribution of Scaled Features (Training Data)', y=1.02)
    plt.tight_layout()
    plt.show()

    # Correlation matrix after preprocessing
    plt.figure(figsize=(14, 10))
    correlation_after = df_processed.corr()
    sns.heatmap(correlation_after, annot=True, cmap='coolwarm', fmt=".2f",
                linewidths=0.5, annot_kws={"size": 8})
    plt.title('Correlation Matrix After Preprocessing')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

else:
    print("Processed dataset not available. Run previous cells.")

"""Tahap ini bertujuan untuk memvalidasi efektivitas dari langkah-langkah persiapan data yang telah dilakukan dan memastikan data benar-benar siap untuk tahap pemodelan.

#### **1. Perbandingan Distribusi Fitur (Sebelum vs. Sesudah)**

- **Metode yang digunakan**
  [cite_start]Menggunakan visualisasi **Histogram** dan **Box Plot** yang ditampilkan berdampingan (*side-by-side*) untuk membandingkan distribusi setiap fitur sebelum dan sesudah tahap pra-pemrosesan (penghapusan duplikat dan penanganan *outlier*).

- **Alasan penggunaan**
  Perbandingan visual secara langsung adalah cara paling efektif untuk memvalidasi hasil dari proses pembersihan data. Metode ini memungkinkan kita untuk segera melihat apakah distribusi data menjadi lebih terpusat, apakah "ekor" distribusi yang panjang (menandakan *skewness*) berkurang, dan apakah *outliers* telah berhasil ditangani.

- **Insight dan Hasil yang didapat**
  - **Efektivitas Penanganan Outlier**: Dari perbandingan **Box Plot**, terlihat jelas bahwa titik-titik *outlier* yang sebelumnya ada di hampir semua fitur (plot merah) telah berhasil dihilangkan pada data setelah pra-pemrosesan (plot biru).
  - [cite_start]**Distribusi Lebih Rapat**: **Histogram** menunjukkan bahwa fitur-fitur yang sebelumnya sangat miring (seperti `residual sugar`, `chlorides`) kini memiliki distribusi yang lebih terpusat dan tidak lagi memiliki "ekor" yang ekstrem.
  - **Validasi Proses**: Secara keseluruhan, visualisasi ini mengonfirmasi bahwa teknik *capping* untuk menangani *outliers* telah berjalan dengan sukses.

---
#### **2. Distribusi Fitur Setelah Scaling**

- **Metode yang digunakan**
  Membuat **Histogram** untuk setiap fitur pada data latih yang telah melalui proses standardisasi menggunakan `StandardScaler`.

- **Alasan penggunaan**
  Tujuan dari visualisasi ini adalah untuk memverifikasi bahwa proses *feature scaling* telah berhasil. Setelah standardisasi, setiap fitur seharusnya memiliki distribusi dengan rata-rata (mean) 0 dan standar deviasi 1. Histogram memungkinkan kita melihat secara visual bahwa pusat distribusi (modus) setiap fitur kini berada di sekitar nilai 0.

- **Insight dan Hasil yang didapat**
  - **Pusat Distribusi**: Semua plot histogram menunjukkan bahwa distribusi setiap fitur kini **terpusat di sekitar nilai 0** pada sumbu horizontal.
  - **Skala Seragam**: Meskipun bentuk asli distribusi (misalnya, miring atau simetris) tetap ada, rentang nilai untuk semua fitur kini seragam, umumnya berada di antara -2 hingga +2. Ini menandakan `StandardScaler` telah bekerja dengan baik dan model tidak akan bias oleh fitur dengan skala nilai yang besar.

---
#### **3. Analisis Korelasi Setelah Pra-pemrosesan**

- **Metode yang digunakan**
  Membuat kembali **Matriks Korelasi** yang divisualisasikan dengan **Heatmap** pada dataset yang telah bersih dari duplikat dan *outliers*.

- **Alasan penggunaan**
  Penting untuk memeriksa kembali korelasi setelah data dibersihkan untuk memastikan bahwa hubungan fundamental antar variabel tidak berubah secara drastis. Ini juga menjadi konfirmasi akhir mengenai fitur-fitur mana yang paling berpengaruh terhadap target sebelum masuk ke tahap pemodelan.

- **Insight dan Hasil yang didapat**
  - **Konsistensi Korelasi**: Pola korelasi secara umum tetap **konsisten** dengan analisis sebelum pra-pemrosesan. Ini adalah hasil yang baik, karena menunjukkan proses pembersihan tidak merusak struktur informasi yang ada di dalam data.
  - **Prediktor Utama Tetap Sama**: `alcohol` (korelasi 0.45 dengan `quality_category`) dan `volatile acidity` (korelasi -0.32) tetap menjadi fitur dengan korelasi terkuat terhadap kualitas, menguatkan statusnya sebagai prediktor penting.
  - **Kesimpulan**: Data yang lebih bersih ini siap untuk pemodelan tanpa khawatir adanya distorsi informasi akibat *outliers* yang ekstrem.

---
#### **4. Kesimpulan Akhir Pra-pemrosesan**

- **Metode yang digunakan**
  Evaluasi kualitatif berdasarkan hasil dari semua metode yang diterapkan pada tahap *Data Preparation* dan *EDA After Preprocessing*.

- **Alasan penggunaan**
  Untuk merangkum semua temuan dan memberikan pernyataan akhir mengenai kesiapan data sebelum melanjutkan ke tahap yang paling krusial, yaitu pemodelan.

- **Insight dan Hasil yang didapat**
  - **Kualitas Data Meningkat**: Proses penghapusan duplikat dan penanganan *outliers* telah berhasil meningkatkan kualitas dan keandalan dataset.
  - **Fitur Siap Digunakan**: Proses *feature scaling* telah berhasil menyeragamkan skala semua fitur, memenuhi prasyarat untuk berbagai algoritma *machine learning*.
  - **Kesiapan Pemodelan**: Dengan data yang bersih, bebas *outlier* ekstrem, dan berskala seragam, dataset kini berada dalam **kondisi optimal** untuk dilanjutkan ke tahap pelatihan dan evaluasi model.

## Modeling
"""

# Modeling
if 'X_train_scaled' in locals():
    print("=== MODELING PHASE ===")

    # Dictionary to store all models and their results
    models = {}
    results = {}

    # Function to evaluate and visualize model performance
    def evaluate_model(model, model_name, X_train, X_test, y_train, y_test):
        # Training predictions
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # Calculate metrics
        train_accuracy = accuracy_score(y_train, y_pred_train)
        test_accuracy = accuracy_score(y_test, y_pred_test)
        test_precision = precision_score(y_test, y_pred_test, average='weighted')
        test_recall = recall_score(y_test, y_pred_test, average='weighted')
        test_f1 = f1_score(y_test, y_pred_test, average='weighted')

        # Store results
        model_results = {
            'model': model,
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'y_pred_test': y_pred_test
        }

        # Print results
        print(f"\n--- {model_name} Results ---")
        print(f"Training Accuracy: {train_accuracy:.4f}")
        print(f"Test Accuracy: {test_accuracy:.4f}")
        print(f"Test Precision (weighted): {test_precision:.4f}")
        print(f"Test Recall (weighted): {test_recall:.4f}")
        print(f"Test F1-score (weighted): {test_f1:.4f}")

        # Classification report
        print(f"\nClassification Report - {model_name}:")
        print(classification_report(y_test, y_pred_test))

        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred_test)
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                    xticklabels=['Not Good', 'Good'], yticklabels=['Not Good', 'Good'])
        plt.title(f'Confusion Matrix - {model_name}')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.show()

        return model_results

    # 1. Logistic Regression (Baseline)
    print("Training Logistic Regression...")
    log_reg = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)
    log_reg.fit(X_train_scaled, y_train)
    results['Logistic Regression'] = evaluate_model(
        log_reg, 'Logistic Regression', X_train_scaled, X_test_scaled, y_train, y_test
    )
    models['Logistic Regression'] = log_reg

    # 2. Random Forest (Default)
    print("\nTraining Random Forest (Default)...")
    rf_default = RandomForestClassifier(random_state=42, n_estimators=100)
    rf_default.fit(X_train_scaled, y_train)
    results['Random Forest (Default)'] = evaluate_model(
        rf_default, 'Random Forest (Default)', X_train_scaled, X_test_scaled, y_train, y_test
    )
    models['Random Forest (Default)'] = rf_default

    # 3. SVM
    print("\nTraining SVM...")
    svm_model = SVC(random_state=42, kernel='rbf')
    svm_model.fit(X_train_scaled, y_train)
    results['SVM'] = evaluate_model(
        svm_model, 'SVM', X_train_scaled, X_test_scaled, y_train, y_test
    )
    models['SVM'] = svm_model

else:
    print("Scaled training data not available. Run previous cells.")

"""Pada tahap ini, tiga algoritma klasifikasi yang berbeda dilatih menggunakan data yang telah dipersiapkan. Performa setiap model dievaluasi menggunakan metrik standar untuk dibandingkan dan dipilih model terbaik sebagai kandidat final.

#### **1. Logistic Regression (Baseline Model)**
- **Metode yang digunakan**
  **Regresi Logistik** digunakan sebagai model dasar. Ini adalah model linear yang menghitung probabilitas sebuah sampel masuk ke dalam kelas tertentu (dalam kasus ini, "Good" atau "Not Good").

- **Alasan penggunaan**
  Regresi Logistik dipilih sebagai *baseline* karena kesederhanaannya, kecepatan trainingnya, dan kemudahannya untuk diinterpretasikan. Model ini memberikan tolok ukur performa yang solid untuk dibandingkan dengan model yang lebih kompleks.

- **Insight dan Hasil yang didapat**
  - **Performa**: Model ini mencapai **Akurasi Tes sebesar 72.43%**, yang merupakan titik awal yang cukup baik.
  - **Generalisasi**: Kesenjangan antara akurasi training (74.7%) dan akurasi tes (72.4%) sangat kecil. Ini menandakan model **tidak mengalami *overfitting*** dan memiliki kemampuan generalisasi yang baik.
  - **Confusion Matrix**: Dari 272 data uji, model ini membuat kesalahan prediksi sebanyak 75 kali (34 *False Positive* + 41 *False Negative*). Kesalahan paling banyak terjadi saat model salah mengklasifikasikan anggur berkualitas "Good" sebagai "Not Good" (41 kasus).

---
#### **2. Random Forest (Default Parameters)**
- **Metode yang digunakan**
  **Random Forest Classifier**, sebuah metode *ensemble* yang membangun banyak *decision tree* secara acak dan menggabungkan hasil prediksi dari semua pohon untuk menghasilkan prediksi akhir yang lebih akurat dan stabil.

- **Alasan penggunaan**
  Random Forest dipilih karena kemampuannya untuk menangkap hubungan non-linear yang kompleks antar fitur tanpa memerlukan asumsi yang ketat tentang distribusi data. Algoritma ini juga umumnya kuat terhadap *outliers*.

- **Insight dan Hasil yang didapat**
  - **Performa**: Model ini menunjukkan peningkatan performa yang signifikan dengan **Akurasi Tes 76.10%**.
  - **Overfitting**: Terdeteksi adanya **overfitting yang signifikan**. Model mencapai akurasi sempurna (100%) pada data training, namun turun menjadi 76.10% pada data tes. Ini menunjukkan model terlalu "menghafal" data training dan perlu dioptimalkan (*tuning*) agar bisa generalisasi lebih baik.
  - **Confusion Matrix**: Dibandingkan Regresi Logistik, Random Forest berhasil mengurangi jumlah total kesalahan prediksi menjadi 65 (29 *False Positive* + 36 *False Negative*), menunjukkan performa klasifikasi yang lebih baik secara keseluruhan.

---
#### **3. Support Vector Machine (SVM)**
- **Metode yang digunakan**
  **Support Vector Classifier (SVC)** dengan kernel **RBF (Radial Basis Function)**. SVM bekerja dengan mencari *hyperplane* (garis pemisah) terbaik yang dapat memaksimalkan jarak (margin) antara dua kelas data. Kernel RBF memungkinkannya menemukan garis pemisah yang non-linear.

- **Alasan penggunaan**
  SVM dengan kernel RBF sangat efektif untuk masalah klasifikasi yang kompleks dan non-linear. Algoritma ini seringkali mampu memberikan akurasi yang tinggi dan memiliki mekanisme regularisasi bawaan yang membuatnya tahan terhadap *overfitting*.

- **Insight dan Hasil yang didapat**
  - **Performa Terbaik**: SVM menghasilkan **Akurasi Tes tertinggi sebesar 77.21%**.
  - **Generalisasi Baik**: Model ini menunjukkan **kemampuan generalisasi yang sangat baik**, dengan selisih akurasi training (80.5%) dan tes (77.2%) yang kecil. Ini menandakan *overfitting* terkontrol dengan baik.
  - **Classification Report**: Laporan klasifikasi menunjukkan performa yang seimbang. Model ini sangat baik dalam mengidentifikasi anggur "Not Good" (*recall* 80%) dan sangat presisi ketika memprediksi anggur "Good" (*precision* 81%). Ini menjadikannya model yang paling andal di antara ketiganya.

### Best Model Selection for Hyperparameter Tuning
"""

# Best Model Selection & Hyperparameter Tuning
if 'results' in locals():
    print("=== BEST MODEL SELECTION ===")

    # Select best model based on F1-score
    best_model_name = max(results.keys(), key=lambda x: results[x]['test_f1'])
    best_f1_score = results[best_model_name]['test_f1']

    print(f"Best performing model: {best_model_name}")
    print(f"Best F1-score: {best_f1_score:.4f}")

    # Hyperparameter tuning for the best model
    print(f"\n=== HYPERPARAMETER TUNING FOR {best_model_name.upper()} ===")

    if 'Random Forest' in best_model_name:
        print("Starting hyperparameter tuning for Random Forest...")

        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None]
        }

        base_model = RandomForestClassifier(random_state=42)

    elif 'Logistic Regression' in best_model_name:
        print("Starting hyperparameter tuning for Logistic Regression...")

        param_grid = {
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga'],
            'max_iter': [1000, 2000]
        }

        base_model = LogisticRegression(random_state=42)

    elif 'SVM' in best_model_name:
        print("Starting hyperparameter tuning for SVM...")

        param_grid = {
            'C': [0.1, 1, 10, 100],
            'kernel': ['linear', 'rbf', 'poly'],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]
        }

        base_model = SVC(random_state=42)

    # Perform Grid Search
    grid_search = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=5,
        scoring='f1_weighted',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X_train_scaled, y_train)

    print(f"\nHyperparameter tuning completed!")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

    # Evaluate tuned model
    best_tuned_model = grid_search.best_estimator_
    tuned_model_name = f"{best_model_name} (Tuned)"

    results[tuned_model_name] = evaluate_model(
        best_tuned_model, tuned_model_name, X_train_scaled, X_test_scaled, y_train, y_test
    )
    models[tuned_model_name] = best_tuned_model

else:
    print("Model results not available. Run previous cells.")

"""Tahap ini bertujuan untuk memilih model dengan performa terbaik berdasarkan hasil evaluasi sebelumnya, lalu mencoba mengoptimalkan performa model tersebut lebih lanjut dengan mencari kombinasi *hyperparameter* yang paling optimal.

#### **1. Pemilihan Model Terbaik (Best Model Selection)**
- **Metode yang digunakan**
  **Analisis komparatif** dari metrik performa (Akurasi, Presisi, Recall, F1-score) ketiga model yang telah dilatih. **F1-score (weighted)** digunakan sebagai metrik utama untuk menentukan model terbaik secara kuantitatif.

- **Alasan penggunaan**
  F1-score dipilih karena merupakan metrik yang menyeimbangkan antara *Precision* (kemampuan model untuk tidak salah melabeli kelas positif) dan *Recall* (kemampuan model untuk menemukan semua sampel positif). Metrik ini lebih robust daripada akurasi, terutama pada dataset yang kelasnya tidak seimbang sempurna.

- **Insight dan Hasil yang didapat**
  - Berdasarkan nilai F1-score tertinggi (0.7722), **SVM** secara objektif terpilih sebagai **model dengan performa terbaik** dibandingkan Logistic Regression dan Random Forest.
  - Pemilihan ini konsisten dengan hasil metrik lainnya, di mana SVM juga unggul dalam hal akurasi dan menunjukkan kemampuan generalisasi yang baik (tidak *overfitting*).

---
#### **2. Tuning Model Terbaik dengan GridSearchCV**
- **Metode yang digunakan**
  **Grid Search Cross Validation (`GridSearchCV`)** diterapkan pada model SVM terpilih. Metode ini secara sistematis menguji berbagai kombinasi *hyperparameter* (`C`, `kernel`, dan `gamma`) menggunakan validasi silang 5-fold (*5-fold cross-validation*) untuk menemukan kombinasi yang menghasilkan F1-score tertinggi.

- **Alasan penggunaan**
  Performa sebuah model seringkali dapat ditingkatkan dengan menyesuaikan *hyperparameter*-nya. `GridSearchCV` adalah metode standar dan menyeluruh untuk mengotomatiskan proses pencarian ini, sementara *cross-validation* memastikan bahwa performa yang diukur lebih stabil dan tidak bergantung pada satu kali pembagian data acak.

- **Insight dan Hasil yang didapat**
  - **Parameter Optimal**: `GridSearchCV` menemukan kombinasi parameter terbaik pada `{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}`.
  - **Penurunan Performa**: Menariknya, setelah model SVM dilatih ulang dengan parameter "terbaik" ini, performanya pada *test set* justru **sedikit menurun**. Akurasi turun dari **77.21%** (model default) menjadi **75.37%** (model yang di-*tuning*).
  - **Analisis Confusion Matrix**: Model yang di-*tuning* membuat total 67 kesalahan (31 FP + 36 FN), lebih banyak dibandingkan model SVM default.
  - **Kesimpulan Tuning**: Dalam kasus ini, proses *hyperparameter tuning* tidak berhasil meningkatkan performa. Hal ini bisa terjadi jika parameter default dari library Scikit-learn memang sudah sangat cocok untuk karakteristik dataset ini, atau karena *tuning* sedikit *overfit* pada data validasi silang.

---
#### **3. Kesimpulan Akhir dan Rekomendasi Model**
- **Metode yang digunakan**
  Perbandingan akhir antara model SVM dengan parameter default dan model SVM yang telah di-*tuning*.

- **Alasan penggunaan**
  Untuk membuat keputusan final berdasarkan bukti empiris dari hasil evaluasi pada data uji yang independen.

- **Insight dan Hasil yang didapat**
  - **Model Final yang Dipilih**: **SVM dengan parameter default** (`C=1`, `kernel='rbf'`, `gamma='scale'`) tetap menjadi pilihan terbaik karena memberikan akurasi dan F1-score tertinggi pada data uji.
  - **Rekomendasi**: Untuk implementasi atau *deployment*, model SVM default adalah kandidat yang paling andal dan berkinerja paling baik berdasarkan eksperimen yang telah dilakukan.

## Final Evaluation and Comparison
"""

# Final Evaluation and Comparison
if 'results' in locals():
    print("=== FINAL MODEL COMPARISON ===")

    # Create comprehensive results dataframe
    comparison_data = []
    for model_name, result in results.items():
        comparison_data.append({
            'Model': model_name,
            'Train Accuracy': result['train_accuracy'],
            'Test Accuracy': result['test_accuracy'],
            'Test Precision': result['test_precision'],
            'Test Recall': result['test_recall'],
            'Test F1-Score': result['test_f1']
        })

    results_df = pd.DataFrame(comparison_data)
    results_df = results_df.round(4)

    print("Complete Model Performance Comparison:")
    print(results_df.to_string(index=False))

    # Visualization of model comparison
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # Test Accuracy
    sns.barplot(data=results_df, x='Model', y='Test Accuracy', ax=axes[0,0], palette='viridis')
    axes[0,0].set_title('Test Accuracy Comparison')
    axes[0,0].tick_params(axis='x', rotation=45)
    for i, v in enumerate(results_df['Test Accuracy']):
        axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center')

    # Test Precision
    sns.barplot(data=results_df, x='Model', y='Test Precision', ax=axes[0,1], palette='plasma')
    axes[0,1].set_title('Test Precision Comparison')
    axes[0,1].tick_params(axis='x', rotation=45)
    for i, v in enumerate(results_df['Test Precision']):
        axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center')

    # Test Recall
    sns.barplot(data=results_df, x='Model', y='Test Recall', ax=axes[1,0], palette='cividis')
    axes[1,0].set_title('Test Recall Comparison')
    axes[1,0].tick_params(axis='x', rotation=45)
    for i, v in enumerate(results_df['Test Recall']):
        axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center')

    # Test F1-Score
    sns.barplot(data=results_df, x='Model', y='Test F1-Score', ax=axes[1,1], palette='magma')
    axes[1,1].set_title('Test F1-Score Comparison')
    axes[1,1].tick_params(axis='x', rotation=45)
    for i, v in enumerate(results_df['Test F1-Score']):
        axes[1,1].text(i, v + 0.01, f'{v:.3f}', ha='center')

    plt.tight_layout()
    plt.show()

    # Overall best model
    final_best_model = results_df.loc[results_df['Test F1-Score'].idxmax(), 'Model']
    final_best_f1 = results_df['Test F1-Score'].max()

    print(f"\n🏆 FINAL BEST MODEL: {final_best_model}")
    print(f"🏆 BEST F1-SCORE: {final_best_f1:.4f}")

    # Performance improvement summary
    if len(results_df) > 1:
        baseline_f1 = results_df.iloc[0]['Test F1-Score']  # Assuming first model is baseline
        improvement = ((final_best_f1 - baseline_f1) / baseline_f1) * 100
        print(f"📈 IMPROVEMENT OVER BASELINE: {improvement:.2f}%")

    # Model complexity vs performance trade-off analysis
    print(f"\n=== MODEL ANALYSIS ===")
    for model_name, result in results.items():
        overfitting = result['train_accuracy'] - result['test_accuracy']
        print(f"{model_name}:")
        print(f"  - Overfitting indicator: {overfitting:.4f}")
        print(f"  - Generalization: {'Good' if overfitting < 0.05 else 'Moderate' if overfitting < 0.1 else 'Poor'}")

else:
    print("Results not available. Run previous cells.")

"""Tahap ini merupakan puncak dari proses pemodelan, di mana semua hasil evaluasi dari setiap model dikumpulkan, dibandingkan secara langsung, dan dianalisis untuk menentukan model pemenang yang paling unggul dan andal.

- **Metode yang digunakan**
  - **Tabel Perbandingan Komprehensif**: Sebuah tabel ringkasan dibuat untuk menampilkan semua metrik evaluasi (*Train Accuracy, Test Accuracy, Precision, Recall, F1-Score*) untuk setiap model yang telah dilatih.
  - **Visualisasi Bar Plot**: Empat buah *bar plot* digunakan untuk membandingkan secara visual kinerja setiap model pada data uji (*test set*) berdasarkan empat metrik utama.
  - **Analisis Generalisasi**: Perhitungan selisih antara akurasi data latih dan data uji (`Train Accuracy - Test Accuracy`) untuk mengukur tingkat *overfitting* setiap model.

- **Alasan penggunaan**
  - **Perbandingan Objektif**: Mengumpulkan semua metrik dalam satu tabel dan visualisasi memungkinkan perbandingan yang adil dan objektif antar model.
  - **Komunikasi Hasil**: *Bar plot* adalah cara yang sangat efektif dan mudah dipahami untuk mengkomunikasikan model mana yang berkinerja terbaik secara visual.
  - **Penilaian Keandalan**: Analisis generalisasi sangat krusial. Model yang baik tidak hanya memiliki akurasi tinggi, tetapi juga harus bisa diandalkan dan tidak *overfitting*, artinya performanya konsisten pada data baru.

- **Insight dan Hasil yang didapat**
  - **Model Performa Terbaik**: Visualisasi dan tabel perbandingan secara konsisten menunjukkan bahwa **SVM (Default)** unggul di semua metrik evaluasi pada data uji, dengan **Test F1-Score tertinggi sebesar 0.7722**. [cite: 1]
  - **Peningkatan dari Baseline**: Model SVM (Default) berhasil memberikan peningkatan performa F1-Score sebesar **6.58%** dibandingkan dengan model *baseline* (Logistic Regression), yang membuktikan bahwa penggunaan model yang lebih kompleks memberikan nilai tambah. [cite: 1]
  - **Analisis Overfitting**:
    - **Random Forest** menunjukkan **generalisasi yang buruk (*Poor*)** karena adanya *overfitting* yang sangat tinggi (selisih akurasi 23.9%). [cite: 1]
    - **Logistic Regression, SVM, dan SVM (Tuned)** menunjukkan **generalisasi yang baik (*Good*)**, dengan selisih akurasi di bawah 5%, menandakan model-model ini lebih stabil dan dapat diandalkan. [cite: 1]
  - **Kesimpulan Akhir**: Meskipun *hyperparameter tuning* telah dilakukan, **SVM dengan parameter default** tetap menjadi model juara. Model ini berhasil memberikan keseimbangan terbaik antara performa prediksi yang tinggi dan kemampuan generalisasi yang solid.

### Model Interpretation and Insights
"""

# Model Interpretation and Insights
print("=== MODEL INTERPRETATION AND INSIGHTS ===")
if 'results_df' in locals() and 'models' in locals():
    # Dapatkan model terbaik berdasarkan F1-score tertinggi pada data tes
    best_model_row = results_df.loc[results_df['Test F1-Score'].idxmax()]
    final_best_model_name = best_model_row['Model']
    final_best_model_obj = models[final_best_model_name]

    print(f"\nModel terbaik adalah: {final_best_model_name} dengan Test F1-Score = {best_model_row['Test F1-Score']:.4f}")
    print(f"Menganalisis fitur penting dari model {final_best_model_name}...")

    # KASUS 1: Jika model memiliki 'feature_importances_' (misal: RandomForest, XGBoost)
    if hasattr(final_best_model_obj, 'feature_importances_'):
        print("Menggunakan metode 'feature_importances_'.")
        importances = final_best_model_obj.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': importances
        }).sort_values('importance', ascending=False)

        # Visualisasi
        plt.figure(figsize=(5, 6))
        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(5), palette='viridis')
        plt.title(f'Top 5 Important Features for {final_best_model_name}')
        plt.xlabel('Importance')
        plt.ylabel('Feature')
        plt.tight_layout()
        plt.show()

    # KASUS 2: Jika model tidak memiliki 'feature_importances_' (misal: SVM, Logistic Regression)
    else:
            print("Menggunakan metode 'Permutation Importance' dengan scoring 'f1_weighted'.")

    # Hitung permutation importance pada data tes yang sudah di-scale
    # Menggunakan 'f1_weighted' sebagai scoring agar sesuai dengan metrik evaluasi utama
    result = permutation_importance(
        final_best_model_obj,
        X_test_scaled,
        y_test,
        n_repeats=10,
        random_state=42,
        n_jobs=-1,
        scoring='f1_weighted'
    )

    # Buat DataFrame dari hasil
    feature_importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance_mean': result.importances_mean,
        'importance_std': result.importances_std
    }).sort_values('importance_mean', ascending=False)

    # Filter fitur yang tidak penting (importance mendekati nol) agar plot lebih bersih
    feature_importance_df = feature_importance_df[feature_importance_df['importance_mean'] > 0.001]

    # --- Visualisasi ---
    if not feature_importance_df.empty:
        plt.figure(figsize=(10, 8))
        sns.barplot(
            x='importance_mean',
            y='feature',
            data=feature_importance_df,
            palette='viridis',
            xerr=feature_importance_df['importance_std']
        )
        plt.title(f'Feature Importance for {final_best_model_name} (Permutation Importance)', fontsize=16)
        plt.xlabel('Mean Importance (F1-Score Drop)', fontsize=12)
        plt.ylabel('Feature', fontsize=12)
        plt.tight_layout()
        plt.show()
    else:
        print("\nTidak ada fitur yang menunjukkan 'importance' signifikan setelah diukur dengan F1-score.")

    print("\nFitur Paling Berpengaruh Teratas:")
    print(feature_importance_df.head())

else:
    print("Variabel 'results_df' atau 'models' tidak ditemukan. Pastikan sel training sudah dijalankan.")

"""Setelah memilih model terbaik, tahap ini bertujuan untuk "membongkar" model tersebut untuk memahami fitur (variabel) mana yang paling berpengaruh dalam membuat prediksi. Ini mengubah model dari sekadar "kotak hitam" menjadi alat yang dapat memberikan wawasan.

- **Metode yang digunakan**
  **Permutation Importance** digunakan untuk mengukur pentingnya setiap fitur pada model **SVM** yang telah terpilih. Metode ini bekerja dengan cara:
  1.  Mengevaluasi performa model (menggunakan F1-Score) pada data uji.
  2.  Mengacak nilai dari **satu fitur** saja, lalu mengevaluasi kembali performa model.
  3.  Penurunan performa yang terjadi setelah fitur diacak dianggap sebagai "tingkat kepentingan" (*importance*) dari fitur tersebut.
  Proses ini diulang beberapa kali untuk memastikan hasilnya stabil.

- **Alasan penggunaan**
  - **Kompatibilitas Model**: SVM tidak memiliki atribut `.feature_importances_` bawaan seperti Random Forest. *Permutation Importance* bersifat *model-agnostic*, artinya dapat digunakan untuk model apa pun, termasuk SVM.
  - **Fokus pada Dampak Nyata**: Metode ini mengukur seberapa besar kontribusi sebuah fitur terhadap performa prediktif model pada data yang belum pernah dilihat (*test set*), memberikan gambaran yang lebih realistis tentang kegunaan fitur tersebut di dunia nyata.
  - **Metrik yang Konsisten**: Menggunakan *scoring* `'f1_weighted'` memastikan bahwa pengukuran kepentingan fitur sejalan dengan metrik yang digunakan untuk memilih model terbaik.

- **Insight dan Hasil yang didapat**
  - **`alcohol` adalah Fitur Paling Dominan**: Seperti yang terlihat pada grafik, `alcohol` adalah fitur yang paling penting dengan selisih yang signifikan. Mengacak nilai alkohol menyebabkan penurunan F1-score rata-rata sebesar **0.106**. Ini mengonfirmasi temuan dari EDA bahwa kadar alkohol adalah prediktor terkuat untuk kualitas anggur.
  - **Pentingnya Keseimbangan Kimia**: Fitur `sulphates` (penting untuk preservasi) dan `volatile acidity` (berkaitan dengan rasa cuka/asam yang tidak diinginkan) menempati peringkat kedua dan ketiga. Ini menunjukkan bahwa selain alkohol, keseimbangan kimia anggur adalah faktor penentu kualitas yang krusial.
  - **`residual sugar` Kurang Berpengaruh**: Fitur `residual sugar` (sisa gula) berada di peringkat paling bawah, menunjukkan bahwa dalam konteks model ini, tingkat kemanisan tidak menjadi faktor pembeda utama antara anggur berkualitas "Good" dan "Not Good".
  - **Validasi EDA**: Hasil ini sangat konsisten dengan analisis korelasi sebelumnya, di mana `alcohol` dan `volatile acidity` juga menunjukkan hubungan terkuat dengan variabel target. Ini memberikan keyakinan bahwa kesimpulan yang ditarik dari data bersifat solid dan dapat diandalkan.
"""